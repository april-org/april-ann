/*
 * This file is part of APRIL-ANN toolkit (A
 * Pattern Recognizer In Lua with Artificial Neural Networks).
 *
 * Copyright 2012, Salvador Espa√±a-Boquera, Adrian Palacios Corella, Francisco
 * Zamora-Martinez
 *
 * The APRIL-ANN toolkit is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License version 3 as
 * published by the Free Software Foundation
 *
 * This library is distributed in the hope that it will be useful, but WITHOUT
 * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 * FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License
 * for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this library; if not, write to the Free Software Foundation,
 * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
 *
 */
#include <cstdio>
#include <cmath>
#include "errorfunc.h"
#include "wrapper.h"

namespace ANN {
  
  MSE::MSE() : zero_epsilon_distance(0.0f) {
  }
  
  // calcula la funcion de error MSE para una muestra, y deja el error
  // producido para poder retropropagarlo en caso de ser necesario
  void MSE::computePatternErrorFunction(FloatGPUMirroredMemoryBlock *output,
					FloatGPUMirroredMemoryBlock *target_output,
					FloatGPUMirroredMemoryBlock *output_error,
					FloatGPUMirroredMemoryBlock *pattern_errors,
					unsigned int output_size,
					const ANNConfiguration &conf) {
    bool use_cuda_flag = conf.use_cuda_flag && (conf.cur_bunch_size > 1);
    doCalculateMSEErrorFunction(output,
				target_output,
				output_error,
				pattern_errors,
				zero_epsilon_distance,
				output_size,
				conf,
				use_cuda_flag);
  }
  
  // calcula el MSE de un batch a partir de la suma de todas las
  // muestras y el numero de muestras
  float MSE::computeBatchErrorFunction(float error_sums,
				       unsigned int num_patterns) {
    return 0.5 * error_sums / num_patterns;
  }

  ///////////////////////////////////////////////////////////

  Tanh::Tanh() {
  }
  
  // calcula la funcion de error Tanh para una muestra, y deja el error
  // producido para poder retropropagarlo en caso de ser necesario
  void Tanh::computePatternErrorFunction(FloatGPUMirroredMemoryBlock *output,
					 FloatGPUMirroredMemoryBlock *target_output,
					 FloatGPUMirroredMemoryBlock *output_error,
					 FloatGPUMirroredMemoryBlock *pattern_errors,
					 unsigned int output_size,
					 const ANNConfiguration &conf) {
    bool use_cuda_flag = conf.use_cuda_flag && (conf.cur_bunch_size > 1);
    doCalculateTanhErrorFunction(output,
				 target_output,
				 output_error,
				 pattern_errors,
				 output_size,
				 conf,
				 use_cuda_flag);
  }
  
  // calcula el Tanh de un batch a partir de la suma de todas las
  // muestras y el numero de muestras
  float Tanh::computeBatchErrorFunction(float error_sums,
					unsigned int num_patterns) {
    return 0.5 * error_sums / num_patterns;
  }

  ///////////////////////////////////////////////////////////

  /*

    MixtureCrossEntropy::MixtureCrossEntropy() :
    EPSILON(NEAR_ZERO), INF(logf(NEAR_ZERO)) {
    }

    // lo mismo que para MSE pero para la crossentropy
    float MixtureCrossEntropy::computePatternErrorFunction(FloatGPUMirroredMemoryBlock *output,
    FloatGPUMirroredMemoryBlock *target_output,
    FloatGPUMirroredMemoryBlock *output_error,
    FloatGPUMirroredMemoryBlock *pattern_errors,
    unsigned int output_size,
    const ANNConfiguration &conf) {
    bool use_cuda_flag = conf.use_cuda_flag && (conf.cur_bunch_size > 1);
    float bunch_error = doCalculateMixtureCrossEntropy(output,
    target_output,
    output_error,
    pattern_errors,
    EPSILON,
    INF,
    output_size,
    conf,
    use_cuda_flag);
    return bunch_error;
    }

    // idem
    float MixtureCrossEntropy::computeBatchErrorFunction(float error_sums,
    unsigned int num_patterns) {
    return -error_sums/num_patterns;
    }

    ///////////////////////////////////////////////////////////
    */
  
  LocalFMeasure::LocalFMeasure(float alpha) : alpha(alpha), N(0) {
  }
  
  // F(a,b) = \alpha G(a,b)/H(a,b)
  // G(a,b) = \sum (1 - a - b + ab)
  // H(a,b) = \sum (2 - a - b)
  //
  // F'(a,b)/a_i = \alpha ( (-1 + b)*H(a,b) + G(a,b) ) / H^2(a,b)
  //
  // Hay que cambiar el signo de la derivada para que vaya en sentido contrario.
  // El algoritmo de descenso por gradiente minimiza la funcion objetivo, y en
  // este caso lo que queremos es maximizar.
  void LocalFMeasure::computePatternErrorFunction(FloatGPUMirroredMemoryBlock *output,
						  FloatGPUMirroredMemoryBlock *target_output,
						  FloatGPUMirroredMemoryBlock *output_error,
						  FloatGPUMirroredMemoryBlock *pattern_errors,
						  unsigned int output_size,
						  const ANNConfiguration &conf) {
    bool use_cuda_flag = conf.use_cuda_flag && (conf.cur_bunch_size > 1);
    doCalculateLocalFMeasureErrorFunction(alpha,
					  output,
					  target_output,
					  output_error,
					  pattern_errors,
					  output_size,
					  conf,
					  use_cuda_flag);
    ++N;
  }
  
  // idem
  float LocalFMeasure::computeBatchErrorFunction(float error_sums,
						 unsigned int num_patterns) {
    // la FMeasure lleva su propia cuenta para calcular la media sobre el numero
    // de conjuntos BUNCH, no sobre el numero de patrones
    float ret = error_sums/N;
    N = 0;
    return ret;
  }

  /*
    ////////////////////////////////////////

    GA::GA() {
    }
  
    // GA2 = ( \sum (1-a_i) (1-b_i) ) ( \sum a_i b_i)
    // GA2/a_i = (b_i - 1)( \sum a_i b_i ) + ( \sum (1 - a_i)(1 - b_i)) b_i
    //
    // Hay que cambiar el signo de la derivada para que vaya en sentido contrario.
    // El algoritmo de descenso por gradiente minimiza la funcion objetivo, y en
    // este caso lo que queremos es maximizar.
    float GA::computePatternErrorFunction(FloatGPUMirroredMemoryBlock *output,
    FloatGPUMirroredMemoryBlock *target_output,
    FloatGPUMirroredMemoryBlock *output_error,
    FloatGPUMirroredMemoryBlock *pattern_errors,
    unsigned int output_size,
    const ANNConfiguration &conf) {
    bool use_cuda_flag = conf.use_cuda_flag && (conf.cur_bunch_size > 1);
    float bunch_error = doCalculateGA(output,
    target_output,
    output_error,
    pattern_errors,
    output_size,
    conf,
    use_cuda_flag);
    return bunch_error;
    }

    // idem
    float GA::computeBatchErrorFunction(float error_sums,
    unsigned int num_patterns) {
    return error_sums/num_patterns;
    }
  */
  ////////////////////////////////////////

  CrossEntropy::CrossEntropy() :
    EPSILON(NEAR_ZERO), INF(logf(NEAR_ZERO)) {
  }

  // lo mismo que para MSE pero para la crossentropy
  void CrossEntropy::computePatternErrorFunction(FloatGPUMirroredMemoryBlock *output,
						 FloatGPUMirroredMemoryBlock *target_output,
						 FloatGPUMirroredMemoryBlock *output_error,
						 FloatGPUMirroredMemoryBlock *pattern_errors,
						 unsigned int output_size,
						 const ANNConfiguration &conf) {
    bool use_cuda_flag = conf.use_cuda_flag && (conf.cur_bunch_size > 1);
    doCalculateCrossEntropyErrorFunction(output,
					 target_output,
					 output_error,
					 pattern_errors,
					 EPSILON,
					 INF,
					 output_size,
					 conf,
					 use_cuda_flag);

  }

  // idem
  float CrossEntropy::computeBatchErrorFunction(float error_sums,
						unsigned int num_patterns) {
    return -error_sums/num_patterns;
  }


  ///////////////////////////////////////////////////////////

  LogisticCrossEntropy::LogisticCrossEntropy() :
    EPSILON(NEAR_ZERO), INF(logf(NEAR_ZERO)) {
  }

  // lo mismo que para MSE pero para la crossentropy
  void LogisticCrossEntropy::computePatternErrorFunction(FloatGPUMirroredMemoryBlock *output,
							 FloatGPUMirroredMemoryBlock *target_output,
							 FloatGPUMirroredMemoryBlock *output_error,
							 FloatGPUMirroredMemoryBlock *pattern_errors,
							 unsigned int output_size,
							 const ANNConfiguration &conf) {
    bool use_cuda_flag = conf.use_cuda_flag && (conf.cur_bunch_size > 1);
    doCalculateLogisticCrossEntropyErrorFunction(output,
						 target_output,
						 output_error,
						 pattern_errors,
						 EPSILON,
						 INF,
						 output_size,
						 conf,
						 use_cuda_flag);
    
  }
  
  // idem
  float LogisticCrossEntropy::computeBatchErrorFunction(float error_sums,
							unsigned int num_patterns) {
    return -error_sums/num_patterns;
  }


  ///////////////////////////////////////////////////////////


  FullCrossEntropy::FullCrossEntropy() :
    EPSILON(NEAR_ZERO), INF(logf(NEAR_ZERO)) {
  }

  // lo mismo que para MSE pero para la crossentropy
  void FullCrossEntropy::computePatternErrorFunction(FloatGPUMirroredMemoryBlock *output,
						     FloatGPUMirroredMemoryBlock *target_output,
						     FloatGPUMirroredMemoryBlock *output_error,
						     FloatGPUMirroredMemoryBlock *pattern_errors,
						     unsigned int output_size,
						     const ANNConfiguration &conf) {
    bool use_cuda_flag = conf.use_cuda_flag && (conf.cur_bunch_size > 1);
    doCalculateFullCrossEntropyErrorFunction(output,
					     target_output,
					     output_error,
					     pattern_errors,
					     EPSILON,
					     INF,
					     output_size,
					     conf,
					     use_cuda_flag);
  }

  // idem
  float FullCrossEntropy::computeBatchErrorFunction(float error_sums,
						    unsigned int num_patterns) {
    return -error_sums/num_patterns;
  }

  ////////////////////////////////////////

  FullLogisticCrossEntropy::FullLogisticCrossEntropy() :
    EPSILON(NEAR_ZERO), INF(logf(NEAR_ZERO)) {
  }

  // lo mismo que para MSE pero para la crossentropy
  void FullLogisticCrossEntropy::computePatternErrorFunction(FloatGPUMirroredMemoryBlock *output,
							     FloatGPUMirroredMemoryBlock *target_output,
							     FloatGPUMirroredMemoryBlock *output_error,
							     FloatGPUMirroredMemoryBlock *pattern_errors,
							     unsigned int output_size,
							     const ANNConfiguration &conf) {
    bool use_cuda_flag = conf.use_cuda_flag && (conf.cur_bunch_size > 1);
    doCalculateFullLogisticCrossEntropyErrorFunction(output,
						     target_output,
						     output_error,
						     pattern_errors,
						     EPSILON,
						     INF,
						     output_size,
						     conf,
						     use_cuda_flag);
  }

  // idem
  float FullLogisticCrossEntropy::computeBatchErrorFunction(float error_sums,
							    unsigned int num_patterns) {
    return -error_sums/num_patterns;
  }

  ////////////////////////////////////////

  
  NormalizedRootMSE::~NormalizedRootMSE() {
  }
  
  float NormalizedRootMSE::computeErrorFromTimeSerie(float *output_ptr,
						     float *target_output_ptr,
						     unsigned int output_size) {
    float target_mean = 0.0f;
    float numerator = 0.0f, denominator = 0.0f;
    for (unsigned int i=0; i<output_size; i++)
      target_mean += target_output_ptr[i];
    target_mean /= output_size;
    for (unsigned int i=0; i<output_size; ++i) {
      float err = (target_output_ptr[i] - output_ptr[i]);
      numerator += err*err;
      float rel = (target_output_ptr[i] - target_mean);
      denominator += rel*rel;
    }
    return sqrtf(numerator/denominator);
  }

  ////////////////////////////////////////

  RootMSE::~RootMSE() {
  }
  
  float RootMSE::computeErrorFromTimeSerie(float *output_ptr,
					   float *target_output_ptr,
					   unsigned int output_size) {
    float numerator = 0.0f;

    for (unsigned int i=0; i<output_size; ++i) {
      float err = (target_output_ptr[i] - output_ptr[i]);
      numerator += err*err;
    }
    return sqrtf(numerator/output_size);
  }

  ////////////////////////////////////////
  
  AbsoluteError::~AbsoluteError() {
  }
  
  float AbsoluteError::computeErrorFromTimeSerie(float *output_ptr,
						 float *target_output_ptr,
						 unsigned int output_size) {
    float error = 0.0f;
    for (unsigned int i=0; i<output_size; ++i)
      error += fabsf(target_output_ptr[i] - output_ptr[i]);
    return error/output_size;
  }
}
