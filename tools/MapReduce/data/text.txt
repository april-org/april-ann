This page describe the utilities to build and train ANNs. Four main sections are
written: a desciprion of ANN concepts in April-ANN, the easy building procedure
for MLPs, the training helpers, and finally the manual way to train any kind of
ANN.

Inspired by other toolkits (as Torch 7 or pyBrain), ANNs are described as a
composition of blocks call ANN components, so one component is a neural network
itself. A list of all available components appears executing:

Nevertheless, the composition procedure will be explained later. An ANN
component is identified by a name string (which will be automatically generated
if not given). The name must be unique. Some components contains weights in
their core, which are estimated by gradient descent algorithm
(backpropagation). Connection weights objects are identified by a weights name
parameter, which could be reused. If two components have the same weights name,
then they share the same connections object.

All components have an input and output size, which defines the number of weights
(if needed) and the fan-in/fan-out of the component. Components need to be
build (build method) once they are constructed. Build procedure allocates
memory for connections and checks input/output sizes of components.

More accurate description is available at `april_help`, but don't be affraid, the next
section presents an abstraction for train MLPs which automatically does a lot of
this work:

The simpliest kind of ANN is a Multilayer Perceptron (MLP) where each layer is
fully connected with the next layer (feed-forward, all-all connections).

The method `generate` returns an special component object, which cannot be
modified. Actually, it is a Lua table formed by an `ann.components.stack`
instance and other information useful to load and save the MLPs, and it
implements wrapper Lua functions to ANN component methods.

As said before, each component has a unique name, and if needed a weights
name. The next code iterates over all components:

The MLP is composed by 9 components, two activation functions (actf1 and actf2),
two bias components (b1 and b2), one stack component which works as a container
(c1), two hyperplane components containing one bias and one dot_product each one
(layer1 and layer2), and finally two dot_product components (w1 and w2) which
contains weight matrixes.

It is also possible to iterate over all weigths names:

So, our MLP contains two bias vectors (b1 and b2, corresponding with b1 and b2
components), and two weights matrixes (w1 and w2, corresponding with w1 and w2
components). All MLPs generated automatically assign this names to its
components and weights.


One time the component is build by using a trainer instance, the trainer exposes
two interesting methods `trainer:component(COMPONENT_NAME_STRING)` which returns
the component given its name, and `trainer:weights(WEIGTHS_NAME_STRING)` which
returns the connection weigths object given its weigths_name attribute.

More info about `trainable.supervised_trainer` doing:

Two save/load schemes are implemented for all-all MLPs. The first is related to
the component all-all (generated throught function `ann.mlp.all_all.generate`).
The second is related to the `trainable.supervised_trainer` object, and will
be detailed in following sections.

This two functions can store and load from a file the component generated via
`ann.mlp.all_all.generate` function. It only works with this kind of object.
The save function has the precondition of a build component. The load function
loads the weights and returns a built component.

Save and load via `trainable` writes to disk the model, weights, loss function,
and bunch size (note that this list could be larger in the future). The object
must be at build state before save, and load returns a built `trainable` object:

The loss function is used to train the ANNs via gradient descent
algorithm. Trainer objects needs an instance of a loss function to perform
training, being a very useful abstraction of standard training procedures.

Detailed information about loss functions is in:

The loss function could be set at trainer constructor, or using the method
set_loss_function:

Three main error functions are implemented: mean square error (MSE), two class
cross-entropy, and multi-class cross-entropy. Note that cross-entropy like
functions are specialized for log_logistic or log_softmax output activation
functions. Almost all the constructors accepts a `SIZE=0` parameter, which
means that the layer has a dynamic size.:

- `ann.loss.mse(SIZE)` returns an instance of the Mean Squared Error
error function for SIZE neurons. It is a quadratic loss function.

- `ann.loss.mae(SIZE)` returns an instance of the Mean Absolute Error
function, for SIZE neurons. It is not a quadratic loss function.

- `ann.loss.cross_entropy(SIZE)` returns an instance of the two-class
cross-entropy. It only works with `log_logistic`
output activation function. It is based on Kullback-Leibler divergence.

- `ann.loss.multi_class_cross_entropy(SIZE)` returns an instance of the
multi-class cross-entropy.  The parameter must be `SIZE>2`, so for two-class
problems only one output unit with cross-entropy is needed. It only works with
`log_logistic` or `log_softmax` output activation function (its better to use
`log_softmax`). It is based on Kullback-Leibler divergence.

Any component could be parametrized to use different hyperparameters. However,
as was stated at the introduction, an ANN component is a composition of several
components, so the parametrization of a parent component implies the same
parametrization for the child components. It is possible to parametrize a child
component without affect the parents. In order to parametrize a component, you
can use `trainer:component(NAME)` to get the component which you want to modify:
