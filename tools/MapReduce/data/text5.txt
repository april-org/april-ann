
Three main error functions are implemented: mean square error (MSE), two class
cross-entropy, and multi-class cross-entropy. Note that cross-entropy like
functions are specialized for log_logistic or log_softmax output activation
functions. Almost all the constructors accepts a `SIZE=0` parameter, which
means that the layer has a dynamic size.:

- `ann.loss.mse(SIZE)` returns an instance of the Mean Squared Error
error function for SIZE neurons. It is a quadratic loss function.

- `ann.loss.mae(SIZE)` returns an instance of the Mean Absolute Error
function, for SIZE neurons. It is not a quadratic loss function.

- `ann.loss.cross_entropy(SIZE)` returns an instance of the two-class
cross-entropy. It only works with `log_logistic`
output activation function. It is based on Kullback-Leibler divergence.

- `ann.loss.multi_class_cross_entropy(SIZE)` returns an instance of the
multi-class cross-entropy.  The parameter must be `SIZE>2`, so for two-class
problems only one output unit with cross-entropy is needed. It only works with
